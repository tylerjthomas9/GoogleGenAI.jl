var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#GoogleGenAI.GoogleProvider","page":"API","title":"GoogleGenAI.GoogleProvider","text":"GoogleProvider(; api_key::String=\"\", base_url::String=\"https://generativelanguage.googleapis.com\", api_version::String=\"v1beta\")\n\nA configuration object used to set up and authenticate requests to the Google Generative Language API.\n\nFields\n\napi_key::String: Your Google API key. If not provided, the constructor will automatically check for GOOGLE_API_KEY or GEMINI_API_KEY environment variables (with GOOGLE_API_KEY taking precedence if both are set).\nbase_url::String: The base URL for the Google Generative Language API. The default is set to \"https://generativelanguage.googleapis.com\".\napi_version::String: The version of the API you wish to access. The default is set to \"v1beta\".\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.SafetySetting","page":"API","title":"GoogleGenAI.SafetySetting","text":"SafetySetting\n\nFields\n\ncategory::String: The type of harmful content to filter. Must be one of:\n\"HARM_CATEGORY_HARASSMENT\"\n\"HARM_CATEGORY_HATE_SPEECH\"\n\"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n\"HARM_CATEGORY_DANGEROUS_CONTENT\"\n\"HARM_CATEGORY_CIVIC_INTEGRITY\"\nthreshold::String: The sensitivity level for blocking content. Must be one of:\n\"BLOCK_NONE\": \tBlock when high probability of unsafe content is detected.\n\"BLOCK_ONLY_HIGH\": Block only content with a high likelihood of harm.\n\"BLOCK_MEDIUM_AND_ABOVE\": Block when medium or high probability of unsafe content\n\"BLOCK_LOW_AND_ABOVE\": Block when low, medium or high probability of unsafe content\n\"HARM_BLOCK_THRESHOLD_UNSPECIFIED\": Threshold is unspecified, block using default threshold\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.ThinkingConfig","page":"API","title":"GoogleGenAI.ThinkingConfig","text":"ThinkingConfig\n\nConfiguration for thinking features in Gemini models.\n\nThe Gemini 2.5 series models use an internal \"thinking process\" during response generation.  This process contributes to their improved reasoning capabilities and helps them use multi-step  planning to solve complex tasks.\n\nFor more information, see: https://ai.google.dev/gemini-api/docs/thinking#set-budget\n\nFields\n\ninclude_thoughts::Bool: Indicates whether to include thoughts in the response. If true, thoughts are returned only if the model supports thought and thoughts are available.\nthinking_budget::Int: Indicates the thinking budget in tokens. This limits the amount of internal thinking the model can perform.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.GenerateContentConfig","page":"API","title":"GoogleGenAI.GenerateContentConfig","text":"GenerateContentConfig\n\nOptional model configuration parameters.\n\nFields\n\nhttp_options=(;): Used to override HTTP request options.\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\nsystem_instruction::Union{Nothing,String,Vector{Dict{Symbol,Any}}}: Instructions for the model.\ntemperature::Union{Nothing,Float64}: Controls the degree of randomness in token selection.\ntop_p::Union{Nothing,Float64}: Selects tokens from most to least probable until the sum of their probabilities equals this value.\ntop_k::Union{Nothing,Float64}: Samples the top_k tokens with the highest probabilities at each step.\ncandidate_count::Union{Nothing,Int}: Number of response variations to return.\nmax_output_tokens::Union{Nothing,Int}: Maximum number of tokens that can be generated.\nstop_sequences::Union{Nothing,Vector{String}}: List of strings that tell the model to stop generating text.\nresponse_logprobs::Union{Nothing,Bool}: Whether to return the log probabilities of chosen tokens.\nlogprobs::Union{Nothing,Int}: Number of top candidate tokens to return log probabilities for.\npresence_penalty::Union{Nothing,Float64}: Penalizes tokens that already appear, increasing diversity.\nfrequency_penalty::Union{Nothing,Float64}: Penalizes tokens that appear repeatedly, increasing diversity.\nseed::Union{Nothing,Int}: Fixed seed for reproducibility; otherwise, a random number is used.\nresponse_mime_type::Union{Nothing,String}: Output response media type.\nresponse_schema::Union{Nothing,Dict{Symbol,Any}}: Schema that the generated candidate text must adhere to.\nrouting_config::Union{Nothing,Dict{Symbol,Any}}: Configuration for model router requests.\nsafety_settings::Union{Nothing,Vector{SafetySetting}}: Safety settings to block unsafe content.\ntools::Union{Nothing,Vector{Any}}: Enables interaction with external systems.   This can include native tools, function declarations, or Functions for automatic schema generation:\nNative tools are defined directly, e.g., Dict(:googleSearch => Dict()), Dict(:codeExecution => Dict()).\nFunction declarations are included using Dict(:functionDeclarations => [declarations]).\nJulia Functions can be passed directly for automatic schema generation.\nHelper functions are available: create_google_search_tool(), create_code_execution_tool(), create_function_tool().\nfunction_declarations::Union{Nothing,Vector{FunctionDeclaration}}: Declarations of functions that the model can call.   For multi-tool scenarios, consider using the tools field instead.\ntool_config::Union{Nothing,ToolConfig} = nothing: Associates model output to a specific function call.\nlabels::Union{Nothing,Dict{String,String}}: User-defined metadata labels.\ncached_content::Union{Nothing,String}: Resource name of a context cache.\nresponse_modalities::Union{Nothing,Vector{String}}: Requested modalities of the response.\nmedia_resolution::Union{Nothing,String}: Specified media resolution.\nspeech_config::Union{Nothing,Dict{Symbol,Any}}: Speech generation configuration.\naudio_timestamp::Union{Nothing,Bool}: Whether to include audio timestamp in the request.\nautomatic_function_calling::Union{Nothing,Dict{Symbol,Any}}: Configuration for automatic function calling.\nthinking_config::Union{Nothing,ThinkingConfig}: Thinking features configuration.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.FunctionCall","page":"API","title":"GoogleGenAI.FunctionCall","text":"FunctionCall\n\nRepresents a function call from the model.\n\nFields\n\nname::String: The name of the function to call.\nargs::Dict{String, Any}: The arguments for the function call.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.FunctionParameter","page":"API","title":"GoogleGenAI.FunctionParameter","text":"FunctionParameter\n\nRepresents a parameter for a function declaration.\n\nFields\n\ntype::String: The type of the parameter (e.g., \"object\", \"string\", \"array\", etc.).\ndescription::Union{String, Nothing}: Optional description of the parameter.\nproperties::Union{Dict{String, Any}, Nothing}: For \"object\" type, defines the properties.\nitems::Union{Dict{String, Any}, Nothing}: For \"array\" type, defines the array items.\nrequired::Union{Vector{String}, Nothing}: For \"object\" type, lists required property names.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.FunctionDeclaration","page":"API","title":"GoogleGenAI.FunctionDeclaration","text":"FunctionDeclaration\n\nRepresents a function that the model can call.\n\nFields\n\nname::String: The name of the function.\ndescription::Union{String, Nothing}: Optional description of the function.\nparameters::FunctionParameter: The parameters schema for the function.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.FunctionCallingConfig","page":"API","title":"GoogleGenAI.FunctionCallingConfig","text":"FunctionCallingConfig\n\nControls how the model uses function declarations.\n\nFields\n\nmode: \"AUTO\" (default), \"ANY\", or \"NONE\"\nallowed_function_names: Optional list of allowed functions when mode is \"ANY\"\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.ToolConfig","page":"API","title":"GoogleGenAI.ToolConfig","text":"ToolConfig\n\nConfiguration for tools behavior.\n\nFields\n\nfunction_calling_config: Configuration for function calling\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.add_function_result_to_conversation","page":"API","title":"GoogleGenAI.add_function_result_to_conversation","text":"add_function_result_to_conversation(conversation::Vector{Dict{Symbol, Any}}, function_name::String, function_result::Any)\n\nAdds a function result to a conversation for multi-turn function calling.\n\nArguments\n\nconversation::Vector{Dict{Symbol, Any}}: The existing conversation.\nfunction_name::String: The name of the function that was called.\nfunction_result::Any: The result returned by the function.\n\nReturns\n\nUpdated conversation vector with the function result added.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.execute_parallel_function_calls","page":"API","title":"GoogleGenAI.execute_parallel_function_calls","text":"execute_parallel_function_calls(\n    function_calls::Vector, \n    functions::Dict{String, <:Function}\n) -> Dict{String, Any}\n\nExecute multiple function calls in parallel and collect their results.\n\nArguments\n\nfunction_calls::Vector: Function calls from the model (can be Any or FunctionCall)\nfunctions::Dict{String, <:Function}: Dictionary of available functions\n\nReturns\n\nDict{String, Any}: Dictionary mapping function names to their results\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.generate_content","page":"API","title":"GoogleGenAI.generate_content","text":"generate_content(provider::AbstractGoogleProvider, model_name::String, prompt::String; image_path::String, config=GenerateContentConfig()) -> NamedTuple\ngenerate_content(api_key::String, model_name::String, prompt::String; image_path::String, config=GenerateContentConfig()) -> NamedTuple\n\ngenerate_content(provider::AbstractGoogleProvider, model_name::String, conversation::Vector{Dict{Symbol,Any}}; config=GenerateContentConfig()) -> NamedTuple\ngenerate_content(api_key::String, model_name::String, conversation::Vector{Dict{Symbol,Any}}; config=GenerateContentConfig()) -> NamedTuple\n\nGenerate content based on a combination of text prompt and an image (optional).\n\nArguments\n\nprovider::AbstractGoogleProvider: The provider instance for API requests.\napi_key::String: Your Google API key as a string. \nmodel_name::String: The model to use for content generation.\nprompt::String: The text prompt to accompany the image.\n\nKeyword Arguments\n\nimage_path::String (optional, legacy): The path to the image file to include in the request.\nimages::AbstractVector (optional): The images to include in the request. Supports NamedTuples with path and mime_type keys.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nNamedTuple: A named tuple containing the following keys:\ncandidates: A vector of dictionaries, each representing a generation candidate.\nsafety_ratings: A dictionary containing safety ratings for the prompt feedback.\ntext: A string representing the concatenated text from all candidates.\nfunction_calls: Optional vector of function calls from the model.\nresponse_status: An integer representing the HTTP response status code.\nfinish_reason: A string indicating the reason why the generation process was finished.\n\n\n\n\n\ngenerate_content(model_name::String, conversation::Vector{Dict{Symbol,Any}}; config=GenerateContentConfig()) -> NamedTuple\n\nGenerate content using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The model to use for content generation.\nconversation::Vector{Dict{Symbol,Any}}: The conversation history.\n\nKeyword Arguments\n\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nNamedTuple: Same as other generate_content functions.\n\n\n\n\n\ngenerate_content(model_name::String, prompt::String; image_path::String=\"\", config=GenerateContentConfig()) -> NamedTuple\n\nGenerate content using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The model to use for content generation.\nprompt::String: The text prompt.\n\nKeyword Arguments\n\nimage_path::String (optional, legacy): The path to the image file to include in the request.\nimages::AbstractVector (optional): The images to include in the request. Supports NamedTuples with path and mime_type keys.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nNamedTuple: Same as other generate_content functions.\n\n\n\n\n\ngenerate_content(model_name::String, contents::AbstractVector; image_path::String=\"\", config=GenerateContentConfig()) -> NamedTuple\n\nGenerate content using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The model to use for content generation.\ncontents::AbstractVector: The contents vector.\n\nKeyword Arguments\n\nimage_path::String (optional, legacy): The path to the image file to include in the request.\nimages::AbstractVector (optional): The images to include in the request. Supports NamedTuples with path and mime_type keys.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nNamedTuple: Same as other generate_content functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.generate_content_stream","page":"API","title":"GoogleGenAI.generate_content_stream","text":"generate_content_stream(provider::AbstractGoogleProvider, model_name::String, prompt::String; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\ngenerate_content_stream(api_key::String, model_name::String, prompt::String; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\n\ngenerate_content_stream(provider::AbstractGoogleProvider, model_name::String, conversation::Vector{Dict{Symbol,Any}}; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\ngenerate_content_stream(api_key::String, model_name::String, conversation::Vector{Dict{Symbol,Any}}; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\n\nGenerate content in a streaming fashion, returning partial results as they become available.\n\nArguments\n\nprovider::AbstractGoogleProvider: The provider instance for API requests.\napi_key::String: Your Google API key as a string. \nmodel_name::String: The model to use for content generation.\nprompt::String: The text prompt to accompany the image.\n\nKeyword Arguments\n\nimage_path::String (optional): The path to the image file to include in the request.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nChannel: A channel that yields partial text responses as they become available. Each item in the channel is a named tuple with the following fields:\ntext::String: The partial text response.\nfinish_reason::Union{String,Nothing}: The reason why generation stopped, if applicable.\nis_final::Bool: Whether this is the final chunk of the response.\n\n\n\n\n\ngenerate_content_stream(model_name::String, conversation::Vector{Dict{Symbol,Any}}; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\n\nGenerate streaming content using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The model to use for content generation.\nconversation::Vector{Dict{Symbol,Any}}: The conversation history.\n\nKeyword Arguments\n\nimage_path::String (optional, legacy): The path to the image file to include in the request.\nimages::AbstractVector (optional): The images to include in the request. Supports NamedTuples with path and mime_type keys.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nChannel: Same as other generatecontentstream functions.\n\n\n\n\n\ngenerate_content_stream(model_name::String, prompt::String; image_path::String=\"\", config=GenerateContentConfig()) -> Channel\n\nGenerate streaming content using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The model to use for content generation.\nprompt::String: The text prompt.\n\nKeyword Arguments\n\nimage_path::String (optional): The path to the image file to include in the request.\nimages::AbstractVector (optional): The images to include in the request. Supports NamedTuples with path and mime_type keys.\nconfig::GenerateContentConfig (optional): Configuration for the generation request.\n\nReturns\n\nChannel: Same as other generatecontentstream functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.count_tokens","page":"API","title":"GoogleGenAI.count_tokens","text":"count_tokens(provider::AbstractGoogleProvider, model_name::String, prompt::String) -> Int\ncount_tokens(api_key::String, model_name::String, prompt::String) -> Int\n\nCalculate the number of tokens generated by the specified model for a given prompt string.\n\nArguments\n\nprovider::AbstractGoogleProvider: The provider instance containing API key and base URL information.\napi_key::String: Your Google API key as a string. \nmodel_name::String: The name of the model to use for generating content. \nprompt::String: The prompt prompt based on which the text is generated.\n\nReturns\n\nInt: The total number of tokens that the given prompt string would be broken into by the specified model's tokenizer.\n\n\n\n\n\ncount_tokens(model_name::String, prompt::String) -> Int\n\nCount tokens using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The name of the model to use for token counting.  \nprompt::String: The prompt to count tokens for.\n\nReturns\n\nInt: The total number of tokens.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.embed_content","page":"API","title":"GoogleGenAI.embed_content","text":"embed_content(provider::AbstractGoogleProvider, model_name::String, prompt::String; http_kwargs=NamedTuple()) -> NamedTuple\nembed_content(api_key::String, model_name::String, prompt::String; http_kwargs=NamedTuple()) -> NamedTuple\nembed_content(provider::AbstractGoogleProvider, model_name::String, prompts::Vector{String}; http_kwargs=NamedTuple()) -> NamedTuple\nembed_content(api_key::String, model_name::String, prompts::Vector{String}; http_kwargs=NamedTuple()) -> NamedTuple\n\nGenerate an embedding for the given prompt text using the specified model.\n\nArguments\n\nprovider::AbstractGoogleProvider: The provider instance containing API key and base URL information.\napi_key::String: Your Google API key as a string. \nmodel_name::String: The name of the model to use for generating content. \nprompt::String: The prompt prompt based on which the text is generated.\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nNamedTuple: A named tuple containing the following keys:\nvalues: A vector of Float64 representing the embedding values for the given prompt (or prompts).\nresponse_status: An integer representing the HTTP response status code.\n\n\n\n\n\nembed_content(provider::AbstractGoogleProvider, model_name::String, prompts::Vector{String}; ...) -> NamedTuple\n\nBatch embedding for multiple prompts.\n\n\n\n\n\nembed_content(model_name::String, prompt::String; http_kwargs=NamedTuple()) -> NamedTuple\n\nGenerate an embedding for the given prompt text using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The name of the model to use for generating content.\nprompt::String: The prompt text to generate embeddings for.\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function.\n\nReturns\n\nNamedTuple: A named tuple containing the embedding values and response status.\n\n\n\n\n\nembed_content(model_name::String, prompts::Vector{String}; http_kwargs=NamedTuple()) -> NamedTuple\n\nGenerate embeddings for multiple prompts using automatic API key detection from environment variables.\n\nArguments\n\nmodel_name::String: The name of the model to use for generating content.\nprompts::Vector{String}: Vector of prompt texts to generate embeddings for.\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function.\n\nReturns\n\nNamedTuple: A named tuple containing the embedding values and response status.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.list_models","page":"API","title":"GoogleGenAI.list_models","text":"list_models(provider::AbstractGoogleProvider) -> Vector{Dict}\nlist_models(api_key::String) -> Vector{Dict}\nlist_models() -> Vector{Dict}\n\nRetrieve a list of available models along with their details from the Google AI API.\n\nArguments\n\nprovider::AbstractGoogleProvider: The provider instance containing API key and base URL information.\napi_key::String: Your Google API key as a string. \n\nReturns\n\nVector{Dict}: A list of dictionaries, each containing details about an available model.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.create_cached_content","page":"API","title":"GoogleGenAI.create_cached_content","text":"create_cached_content(\n    provider::AbstractGoogleProvider,\n    model_name::String,\n    content::Union{String,Vector{Dict{Symbol,Any}},Dict{String,Any}};\n    ttl::String=\"300s\",\n    system_instruction::String=\"\",\n    http_kwargs=NamedTuple()\n) -> JSON3.Object\ncreate_cached_content(\n    api_key::String,\n    model_name::String,\n    content::Union{String,Vector{Dict{Symbol,Any}},Dict{String,Any}};\n    ttl::String=\"300s\",\n    system_instruction::String=\"\",\n    http_kwargs=NamedTuple()\n) -> JSON3.Object\n\nCreate a cached content resource that can be reused in subsequent requests.\n\nArguments\n\nprovider::AbstractGoogleProvider or api_key::String: The provider instance for API requests or your Google API key as a string.\nmodel_name::String: The model to use (e.g. \"gemini-1.5-flash-001\").\ncontent::Union{String,Vector{Dict{Symbol,Any}},Dict{String,Any}}: The content to cache, which can be a single string, an array of conversation messages, or a raw content dictionary.\nttl::String: Time-to-live duration for the cache. Defaults to \"300s\".\nsystem_instruction::String: An optional system instruction for the model.\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nJSON3.Object: A JSON object containing the metadata of the created cached content resource, including its cache name.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.list_cached_content","page":"API","title":"GoogleGenAI.list_cached_content","text":"list_cached_content(provider::AbstractGoogleProvider; http_kwargs=NamedTuple()) -> JSON3.Array\nlist_cached_content(api_key::String; http_kwargs=NamedTuple()) -> JSON3.Array\n\nLists the cache metadata for all your cached content. (Does not return the cached content itself.)\n\nArguments\n\nprovider::AbstractGoogleProvider or api_key::String: The provider instance for API requests or your Google API key as a string.\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nJSON3.Array: A JSON array of objects, where each object represents a cached content resource's metadata.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.get_cached_content","page":"API","title":"GoogleGenAI.get_cached_content","text":"get_cached_content(provider::AbstractGoogleProvider, cache_name::String; http_kwargs=NamedTuple()) -> JSON3.Object\nget_cached_content(api_key::String, cache_name::String; http_kwargs=NamedTuple()) -> JSON3.Object\n\nRetrieve the metadata for a single cached content resource by its resource name.\n\nArguments\n\nprovider::AbstractGoogleProvider or api_key::String: The provider instance for API requests or your Google API key as a string.\ncache_name::String: The full resource name of the cached content (e.g. \"cachedContents/12345\").\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nJSON3.Object: A JSON object containing the metadata for the specified cached content.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.update_cached_content","page":"API","title":"GoogleGenAI.update_cached_content","text":"update_cached_content(provider::AbstractGoogleProvider, cache_name::String, ttl::String; http_kwargs=NamedTuple()) -> JSON3.Object\nupdate_cached_content(api_key::String, cache_name::String, ttl::String, http_kwargs=NamedTuple()) -> JSON3.Object\n\nUpdate the TTL of an existing cached content resource. Attempts to change other fields are not supported.\n\nArguments\n\nprovider::AbstractGoogleProvider or api_key::String: The provider instance for API requests or your Google API key as a string.\ncache_name::String: The full resource name of the cached content (e.g. \"cachedContents/xyz123\").\nttl::String: The new time-to-live value. Defaults to \"600s\".\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nJSON3.Object: A JSON object containing the updated metadata for the cached content.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.delete_cached_content","page":"API","title":"GoogleGenAI.delete_cached_content","text":"delete_cached_content(provider::AbstractGoogleProvider, cache_name::String; http_kwargs=NamedTuple()) -> Int\ndelete_cached_content(api_key::String, cache_name::String; http_kwargs=NamedTuple()) -> Int\n\nDelete a cached content resource by its resource name.\n\nArguments\n\nprovider::AbstractGoogleProvider or api_key::String: The provider instance for API requests or your Google API key as a string.\ncache_name::String: The full resource name of the cached content (e.g. \"cachedContents/xyz123\").\n\nHTTP Kwargs\n\nAll keyword arguments supported by the HTTP.request function. Documentation can be found here: https://juliaweb.github.io/HTTP.jl/stable/reference/#HTTP.request.\n\nReturns\n\nInt: The HTTP status code of the deletion request.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.upload_file","page":"API","title":"GoogleGenAI.upload_file","text":"upload_file(provider::AbstractGoogleProvider, file_path::String; display_name::String=\"\", mime_type::String=\"application/octet-stream\", http_kwargs=NamedTuple()) -> JSON3.Object\n\nUploads a file using the resumable upload protocol.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.get_file","page":"API","title":"GoogleGenAI.get_file","text":"get_file(provider::AbstractGoogleProvider, file_name::String; http_kwargs=NamedTuple()) -> Any\n\nRetrieves metadata for the file specified by its resource name (e.g. \"files/abc-123\").\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.list_files","page":"API","title":"GoogleGenAI.list_files","text":"list_files(provider::AbstractGoogleProvider; page_size::Int=10, page_token::String=\"\", http_kwargs=NamedTuple()) -> JSON3.Array\n\nLists file metadata for files owned by your project. Use page_size and page_token for pagination.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.delete_file","page":"API","title":"GoogleGenAI.delete_file","text":"delete_file(provider::AbstractGoogleProvider, file_name::String; http_kwargs=NamedTuple()) -> Int\n\nDeletes the file specified by its resource name (e.g. \"files/abc-123\") and returns the HTTP status code.\n\n\n\n\n\n","category":"function"},{"location":"api/#GoogleGenAI.ToolType","page":"API","title":"GoogleGenAI.ToolType","text":"ToolType\n\nEnum representing the types of native tools supported by the Gemini API.\n\nValues\n\nGOOGLE_SEARCH: Represents the Google Search tool functionality that allows the model to search for information.\nCODE_EXECUTION: Represents the Code Execution tool which allows the model to execute code snippets.\nFUNCTION_CALLING: Represents the Function Calling capability that enables the model to call user-defined functions.\n\n\n\n\n\n","category":"type"},{"location":"api/#GoogleGenAI.is_native_tool","page":"API","title":"GoogleGenAI.is_native_tool","text":"is_native_tool(tool::Dict{Symbol, Any}) -> Tuple{Bool, Union{ToolType, Nothing}}\n\nCheck if a tool dictionary represents a native tool and identify its type.\n\nArguments\n\ntool::Dict{Symbol, Any}: The tool dictionary to check\n\nReturns\n\nTuple{Bool, Union{ToolType, Nothing}}: A tuple with a boolean indicating if it's a native tool and the type of the tool if it is native, or nothing if it's not.\n\n\n\n\n\n","category":"function"},{"location":"#GoogleGenAI.jl-Docs","page":"Home","title":"GoogleGenAI.jl Docs","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia wrapper to the Google generative AI API. For API functionality, see reference documentation.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"From source:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(url=\"https://github.com/tylerjthomas9/GoogleGenAI.jl/\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]  # enters the pkg interface\nPkg> add https://github.com/tylerjthomas9/GoogleGenAI.jl/","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#API-Key-Setup","page":"Home","title":"API Key Setup","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use the Gemini API, you need an API key. You can create a key for free with a few clicks in Google AI Studio. For more details, see the API key documentation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are several ways to provide your API key:","category":"page"},{"location":"#Option-1:-Environment-Variables-(Recommended)","page":"Home","title":"Option 1: Environment Variables (Recommended)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Set your API key as an environment variable GOOGLE_API_KEY or GEMINI_API_KEY (if both are set, GOOGLE_API_KEY takes precedence):","category":"page"},{"location":"","page":"Home","title":"Home","text":"export GOOGLE_API_KEY=\"your-api-key-here\"","category":"page"},{"location":"#Option-2:-Explicit-API-Key-as-String","page":"Home","title":"Option 2: Explicit API Key as String","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pass the API key directly as a string parameter:","category":"page"},{"location":"","page":"Home","title":"Home","text":"api_key = \"your-api-key-here\"\nresponse = generate_content(api_key, model, prompt)","category":"page"},{"location":"#Option-3:-GoogleProvider-with-API-Key","page":"Home","title":"Option 3: GoogleProvider with API Key","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Create a GoogleProvider instance with your API key:","category":"page"},{"location":"","page":"Home","title":"Home","text":"provider = GoogleProvider(; api_key=\"your-api-key-here\")\nresponse = generate_content(provider, model, prompt)","category":"page"},{"location":"#Generate-Content","page":"Home","title":"Generate Content","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nmodel = \"gemini-2.5-flash\"\nprompt = \"Hello\"\nresponse = generate_content(model, prompt)\nprintln(response.text)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gemini API config:","category":"page"},{"location":"","page":"Home","title":"Home","text":"config = GenerateContentConfig(; max_output_tokens=50)\nresponse = generate_content(model, prompt; config)\nprintln(response.text)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Single image input (legacy):","category":"page"},{"location":"","page":"Home","title":"Home","text":"prompt = \"What is this image?\"\nimage_path = \"test/input/example.jpg\"\nresponse = generate_content(model, prompt; image_path)\nprintln(response.text)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Multi image input:","category":"page"},{"location":"","page":"Home","title":"Home","text":"prompt = \"What is this image?\"\nimage_path = \"test/input/example.jpg\"\nimages = [(path=image_path,), (path=image_path, mime_type=\"image/png\")]\nresponse = generate_content(model, prompt; images)\nprintln(response.text)","category":"page"},{"location":"#Multi-turn-conversations","page":"Home","title":"Multi-turn conversations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nconfig = GenerateContentConfig(; max_output_tokens=50)\nmodel = \"gemini-2.5-flash\"\nconversation = [\n    Dict(:role => \"user\", :parts => [Dict(:text => \"When was Julia 1.0 released?\")])\n]\n\nresponse = generate_content( model, conversation; config)\npush!(conversation, Dict(:role => \"model\", :parts => [Dict(:text => response.text)]))\nprintln(\"Model: \", response.text) \n\npush!(conversation, Dict(:role => \"user\", :parts => [Dict(:text => \"Who created the language?\")]))\nresponse = generate_content(model, conversation; config)\nprintln(\"Model: \", response.text)","category":"page"},{"location":"#Streaming-Content-Generation","page":"Home","title":"Streaming Content Generation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nmodel = \"gemini-2.5-flash\"\nprompt = \"Why is the sky blue?\"\n\nstream = generate_content_stream(model, prompt)\n\nfor chunk in stream\n    print(chunk.text)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"For multi-turn conversations with streaming:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nprovider = GoogleProvider()\nmodel = \"gemini-2.5-flash\"\nconversation = [\n    Dict(:role => \"user\", :parts => [Dict(:text => \"Write a short poem about Julia programming language\")])\n]\n\nprintln(\"Generating first response...\")\nstream = generate_content_stream(provider, model, conversation)\n\nfor chunk in stream\n    println(\"Response: \", chunk.text)\nend","category":"page"},{"location":"#Generate/Edit-Images","page":"Home","title":"Generate/Edit Images","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Generate image using Gemini:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nconfig = GenerateContentConfig(\n    response_modalities=[\"Text\", \"Image\"]\n)\n\nprompt = (\"Hi, can you create a 3d rendered image of a pig \"*\n            \"with wings and a top hat flying over a happy \"*\n            \"futuristic scifi city with lots of greenery?\")\n\nresponse = generate_content(\n    \"gemini-2.5-flash-image-preview\",\n    prompt;\n    config\n);\n\nif !isempty(response.images)\n    open(\"gemini-native-image.png\", \"w\") do io\n        write(io, response.images[1].data)\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Edit image with Gemini:","category":"page"},{"location":"","page":"Home","title":"Home","text":"image_path = \"gemini-native-image.png\"\n\nmodel = \"gemini-2.5-flash-image-preview\"\nprompt = \"Make the pig a llama\"\nresponse = generate_content(\n    model,\n    prompt;\n    image_path,\n    config\n);\n\nif !isempty(response.images)\n    open(\"gemini-native-image-edited.png\", \"w\") do io\n        write(io, response.images[1].data)\n    end\nend","category":"page"},{"location":"#Count-Tokens","page":"Home","title":"Count Tokens","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nmodel = \"gemini-2.5-flash\"\nn_tokens = count_tokens(model, \"The Julia programming language\")\nprintln(n_tokens)","category":"page"},{"location":"","page":"Home","title":"Home","text":"outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"4","category":"page"},{"location":"#Create-Embeddings","page":"Home","title":"Create Embeddings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nembeddings = embed_content(\"text-embedding-004\", \"Hello\")\nprintln(size(embeddings.values))","category":"page"},{"location":"","page":"Home","title":"Home","text":"outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"(768,)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nembeddings = embed_content(\"text-embedding-004\", [\"Hello\", \"world\"])\nprintln(embeddings.response_status)\nprintln(size(embeddings.values[1]))\nprintln(size(embeddings.values[2]))","category":"page"},{"location":"","page":"Home","title":"Home","text":"outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"200\n(768,)\n(768,)","category":"page"},{"location":"#List-Models","page":"Home","title":"List Models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nmodels = list_models()\nfor m in models\n    if \"generateContent\" in m[:supported_generation_methods]\n        println(m[:name])\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"...\ngemini-2.5-pro-preview-03-25\ngemini-2.5-flash-preview-05-20\ngemini-2.5-flash\ngemini-2.5-flash-lite-preview-06-17\ngemini-2.5-pro-preview-05-06\ngemini-2.5-pro-preview-06-05\ngemini-2.5-pro\ngemini-2.0-flash-exp\ngemini-2.0-flash\ngemini-2.0-flash-001\ngemini-2.0-flash-exp-image-generation\ngemini-2.0-flash-lite-001\ngemini-2.0-flash-lite\ngemini-2.0-flash-preview-image-generation\ngemini-2.0-flash-lite-preview-02-05\ngemini-2.0-flash-lite-preview\ngemini-2.0-pro-exp\ngemini-2.0-pro-exp-02-05\ngemini-exp-1206\ngemini-2.0-flash-thinking-exp-01-21\ngemini-2.0-flash-thinking-exp\ngemini-2.0-flash-thinking-exp-1219\ngemini-2.5-flash-preview-tts\ngemini-2.5-pro-preview-tts\nlearnlm-2.0-flash-experimental\ngemma-3-1b-it\ngemma-3-4b-it\ngemma-3-12b-it\ngemma-3-27b-it\ngemma-3n-e4b-it\ngemma-3n-e2b-it\ngemini-2.5-flash-lite\ngemini-2.5-flash-image-preview","category":"page"},{"location":"#Safety-Settings","page":"Home","title":"Safety Settings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"More information about the safety settings can be found here.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nsafety_settings = [\n    SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"),\n    SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_ONLY_HIGH\"),\n    SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_MEDIUM_AND_ABOVE\"),\n    SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n    SafetySetting(category=\"HARM_CATEGORY_CIVIC_INTEGRITY\", threshold=\"OFF\"),\n]\nmodel = \"gemini-2.5-flash\"\nprompt = \"Hello\"\nconfig = GenerateContentConfig(; safety_settings)\nresponse = generate_content(model, prompt; config)","category":"page"},{"location":"#Thinking","page":"Home","title":"Thinking","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Gemini 2.5 series models use an internal \"thinking process\" during response generation. This process contributes to their improved reasoning capabilities and helps them use multi-step planning to solve complex tasks. This thinking can be limited by setting the thinking_budget. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nthinking_config = ThinkingConfig(; thinking_budget=100, include_thoughts=true)\nconfig = GenerateContentConfig(;\n    thinking_config\n)\nmodel = \"gemini-2.5-flash\"\nresponse = generate_content(model, \"Hello\"; config)","category":"page"},{"location":"#Content-Caching","page":"Home","title":"Content Caching","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"List models that support content caching:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\nmodels = list_models()\nfor m in models\n    if \"createCachedContent\" in m[:supported_generation_methods]\n        println(m[:name])\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"gemini-2.5-pro-preview-03-25\ngemini-2.5-flash-preview-04-17\ngemini-2.5-flash-preview-05-20\ngemini-2.5-flash\ngemini-2.5-flash-preview-04-17-thinking\ngemini-2.5-flash-lite-preview-06-17\ngemini-2.5-pro-preview-05-06\ngemini-2.5-pro-preview-06-05\ngemini-2.5-pro\ngemini-2.0-flash\ngemini-2.0-flash-001\ngemini-2.5-flash-lite-001\ngemini-2.5-flash-lite\ngemini-2.5-flash-lite-preview-02-05\ngemini-2.5-flash-lite-preview\ngemini-2.0-pro-exp\ngemini-2.0-pro-exp-02-05\ngemini-exp-1206\ngemini-2.0-flash-thinking-exp-01-21\ngemini-2.0-flash-thinking-exp\ngemini-2.0-flash-thinking-exp-1219","category":"page"},{"location":"","page":"Home","title":"Home","text":"Cache content to reuse it across multiple requests:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\n# API key is automatically loaded from environment variable\nprovider = GoogleProvider()\nmodel = \"gemini-2.5-flash\"\n\n# Create cached content\ntext = read(\"test/input/example.txt\", String)\ncache_result = create_cached_content(\n    provider,\n    model,\n    text,\n    ttl=\"90s\", # Cache for 90 seconds\n)\n\n# Now generate content that references the cached content.\nprompt = \"Please summarize this document\"\nconfig = GenerateContentConfig(; cached_content=cache_result.name)\nresponse = generate_content(\n    provider,\n    model,\n    prompt;\n    config\n)\nprintln(response.text)\n\n\n# list all cached content\nlist_result = list_cached_content(provider)\n# get details of a specific cache\nget_result = get_cached_content(provider, cache_result.name)\n# update the TTL of a specific cache\nupdate_result = update_cached_content(provider, cache_result.name, \"90s\") \n# delete a specific cache\ndelete_cached_content(provider, cache_result.name)","category":"page"},{"location":"#Files","page":"Home","title":"Files","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Files are only supported in Gemini Developer API.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\n# upload file\nfile_path = \"test/input/example.jpg\"\nupload_result = upload_file(\n    file_path; display_name=\"Test File\",\n)\n\n# generate content with file\nmodel = \"gemini-2.5-flash\"\nprompt = \"What is this image?\"\ncontents = [prompt, upload_result]\nresponse = generate_content(\n    model,\n    contents;\n)\nprintln(response.text)\n\n# Get file metadata\nget_result = get_file(upload_result[:name])\n\n# List files\nlist_result = list_files()\n\n# Delete file\ndelete_file(upload_result[:name])","category":"page"},{"location":"#Structured-Generation","page":"Home","title":"Structured Generation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Json ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\nusing JSON3\n\nmodel = \"gemini-2.5-flash\"\n\n# Define a JSON schema for an Array of Objects\n# Each object has \"recipe_name\" (a String) and \"ingredients\" (an Array of Strings).\nschema = Dict(\n    :type => \"ARRAY\",\n    :items => Dict(\n        :type => \"OBJECT\",\n        :properties => Dict(\n            :recipe_name => Dict(:type => \"STRING\"),\n            :ingredients => Dict(\n                :type  => \"ARRAY\",\n                :items => Dict(:type => \"STRING\")\n            )\n        ),\n        :propertyOrdering => [\"recipe_name\", \"ingredients\"]\n    )\n)\n\nconfig = GenerateContentConfig(\n    response_mime_type = \"application/json\",\n    response_schema    = schema,\n)\n\nprompt = \"List a few popular cookie recipes with exact amounts of each ingredient.\"\nresponse = generate_content(model, prompt; config=config)\njson_string = response.text\nrecipes = JSON3.read(json_string)\nprintln(recipes)","category":"page"},{"location":"","page":"Home","title":"Home","text":"outputs","category":"page"},{"location":"","page":"Home","title":"Home","text":"JSON3.Object[{\n   \"recipe_name\": \"Classic Chocolate Chip Cookies\",\n   \"ingredients\": [\n                    \"1 cup (2 sticks) unsalted butter, softened\",\n                    \"3/4 cup granulated sugar\",\n                    \"3/4 cup packed light brown sugar\",\n                    \"2 large eggs\",\n                    \"1 teaspoon vanilla extract\",\n                    \"2 1/4 cups all-purpose flour\",\n                    \"1 teaspoon baking soda\",\n                    \"1/2 teaspoon salt\",\n                    \"1 cup (6 oz) semi-sweet chocolate chips\"\n                  ]\n}, {\n   \"recipe_name\": \"Soft Oatmeal Raisin Cookies\",\n   \"ingredients\": [\n                    \"1 cup (2 sticks) unsalted butter, softened\",\n                    \"1 cup packed light brown sugar\",\n                    \"1/2 cup granulated sugar\",\n                    \"2 large eggs\",\n                    \"1 teaspoon vanilla extract\",\n                    \"1 1/2 cups all-purpose flour\",\n                    \"1 teaspoon baking soda\",\n                    \"1/2 teaspoon ground cinnamon\",\n                    \"1/2 teaspoon salt\",\n                    \"3 cups rolled oats\",\n                    \"1 cup raisins\"\n                  ]\n}, {\n   \"recipe_name\": \"Simple Peanut Butter Cookies\",\n   \"ingredients\": [\n                    \"1 cup (2 sticks) unsalted butter, softened\",\n                    \"1 cup creamy peanut butter\",\n                    \"1 cup packed light brown sugar\",\n                    \"1 cup granulated sugar\",\n                    \"2 large eggs\",\n                    \"1 teaspoon vanilla extract\",\n                    \"2 1/2 cups all-purpose flour\",\n                    \"1 teaspoon baking soda\",\n                    \"1/2 teaspoon salt\"\n                  ]\n}]","category":"page"},{"location":"#Code-Generation","page":"Home","title":"Code Generation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\n\ntools = [Dict(:code_execution => Dict())]\nconfig = GenerateContentConfig(; tools)\n\nmodel = \"gemini-2.5-flash\"\nprompt = \"Write a function to calculate the factorial of a number.\"\nresponse = generate_content(model, prompt; config=config)\nprintln(response.text)","category":"page"},{"location":"#Function-Calling","page":"Home","title":"Function Calling","text":"","category":"section"},{"location":"#Manually-declare-and-invoke-a-function-for-function-calling","page":"Home","title":"Manually declare and invoke a function for function calling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using GoogleGenAI\nusing JSON3\n\n# Step 1: Create the initial user message\nuser_message = Dict(\n    :role => \"user\", \n    :parts => [Dict(:text => \"What's the weather like in Paris?\")]\n)\n\n# Step 2: Define your function declaration\nweather_function = FunctionDeclaration(\n    \"get_weather\", \n    \"Get current weather information for a location\",\n    Dict{String, Any}(\n        \"type\" => \"object\",\n        \"properties\" => Dict{String, Any}(\n            \"location\" => Dict{String, Any}(\n                \"type\" => \"string\",\n                \"description\" => \"City or location to get weather for\"\n            ),\n            \"unit\" => Dict{String, Any}(\n                \"type\" => \"string\",\n                \"description\" => \"Temperature unit (celsius or fahrenheit)\",\n                \"enum\" => [\"celsius\", \"fahrenheit\"]\n            )\n        ),\n        \"required\" => [\"location\"]\n    )\n)\n\n# Step 3: Configure the model to force function calling\nfc_config = FunctionCallingConfig(mode=\"ANY\")\ntool_config = ToolConfig(function_calling_config=fc_config)\nconfig = GenerateContentConfig(\n    function_declarations=[weather_function],\n    tool_config=tool_config,\n    temperature=0.2\n)\n\n# Step 4: Get the initial response from the model, which should be a function call\nresponse = generate_content(\n    \"gemini-2.5-flash\",\n    [user_message];\n    config=config\n)\n\n# Step 5: Extract the function call details and construct the model's turn\nfunction_name = response.function_calls[1].name\nargs = response.function_calls[1].args\nmodel_message = Dict(\n    :role => \"model\",\n    :parts => [\n        Dict(\n            :functionCall => Dict(\n                :name => function_name,\n                :args => args\n            )\n        )\n    ]\n)\nprintln(\"\\nModel message: \", JSON3.write(model_message))\n\n# Step 6: Execute the function (simulated) and create the function's response message\n# In a real app, you would call your actual get_weather function here.\nweather_result = Dict(\n    \"temperature\" => 18,\n    \"condition\" => \"Sunny\",\n    \"humidity\" => 65\n)\n\nfunction_message = Dict(\n    :role => \"function\",\n    :parts => [\n        Dict(\n            :functionResponse => Dict(\n                :name => function_name,\n                :response => weather_result\n            )\n        )\n    ]\n)\nprintln(\"\\nFunction message: \", JSON3.write(function_message))\n\n# Step 7: Assemble the full conversation history\nconversation_history = [\n    user_message,\n    model_message,\n    function_message\n]\n\n# Step 8: Get the final, natural language response from the model\nfinal_response = generate_content(\n    \"gemini-2.5-flash\",\n    conversation_history\n)\n\nprintln(\"\\nFinal response: $(final_response.text)\")","category":"page"}]
}
